{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jacky Zhao\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import re, string\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    " \n",
    "import math\n",
    "\n",
    "# set seaborn settings\n",
    "sns.set()\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True # set lines\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import credentials as creds\n",
    "import helpers as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# https://praw.readthedocs.io/en/latest/getting_started/\n",
    "# https://www.reddit.com/dev/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CLIENT_ID = creds.client_id()\n",
    "CLIENT_SECRET_KEY = creds.client_secret_key()\n",
    "\n",
    "\n",
    "r = praw.Reddit(client_id = CLIENT_ID,\n",
    "                client_secret = CLIENT_SECRET_KEY,\n",
    "                user_agent = 'RedditorMatch')\n",
    "\n",
    "print(r.read_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the scraped datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_subreddits = [\"mizzou\", \"umich\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar(matrix, index, top_n = 5):\n",
    "    cosine_similarities = linear_kernel(matrix[index: index + 1], matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDf(subreddit_name):\n",
    "    filePath = \"data/\" + subreddit_name + \".csv\"\n",
    "    df = pd.read_csv(filePath, encoding = \"ISO-8859-1\")\n",
    "    print(\"--- Retrieved\", len(df), \"corpuses/corpi(?) for\", subreddit_name)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(corpus):\n",
    "    newCorpus = []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for c in corpus:\n",
    "        tokens = nltk.word_tokenize(c)\n",
    "        new_c = ' '.join(stemmer.stem(t) for t in tokens)\n",
    "        newCorpus.append(new_c)\n",
    "        \n",
    "    return(newCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findMatches(your_username, subreddit_name, redditInstance):\n",
    "    corpus = []    \n",
    "    corpusDf = getDf(subreddit_name)\n",
    "    corpusDf[\"Comments\"].apply(lambda row: corpus.append(str(row)))\n",
    "    \n",
    "    corpus = stem(corpus)\n",
    "    \n",
    "    your_comments = h.getUserComments(your_username, redditInstance)\n",
    "    your_comments = stem(your_comments)    \n",
    "    your_comments = \" \".join(your_comments)\n",
    "    \n",
    "    corpus.insert(0, your_comments)\n",
    "    \n",
    "    print(\"--- Creating Tfidf vector...\")\n",
    "    \n",
    "    myStopWords = stopwords.words('english')\n",
    "    myStopWords = text.ENGLISH_STOP_WORDS.union(myStopWords)\n",
    "    \n",
    "    tf = TfidfVectorizer(analyzer = \"word\", \n",
    "                            ngram_range = (1, 3),\n",
    "                            min_df = 0, \n",
    "                            stop_words = myStopWords)\n",
    "    \n",
    "    print(\"--- Fitting the matrix...\")\n",
    "    matrix = tf.fit_transform(corpus)\n",
    "    results = []\n",
    "    \n",
    "    for index, score in find_similar(matrix, 0):        \n",
    "        index = index - 1 # because we prepended our comments onto the corpus, the index number was shifted by 1.\n",
    "        user = corpusDf.iloc[index, 0]\n",
    "        results.append(user)\n",
    "        print(\"...\")\n",
    "        print(\"...\")\n",
    "        print(\"Score:\", score, \"| Username:\", user)\n",
    "        print(\"=========================================================\")\n",
    "        \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Retrieved 288 corpuses/corpi(?) for mizzou\n"
     ]
    }
   ],
   "source": [
    "matches = findMatches(\"Max_W_\", scraped_subreddits[0], r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRedditorInfo(redditor_name, r):\n",
    "    user = r.redditor(redditor_name)\n",
    "    top = user.comments.top(limit = 1000)\n",
    "    hot = user.comments.hot(limit = 1000)\n",
    "    contro = user.comments.controversial(limit = 1000)\n",
    "    \n",
    "    subreddit = []\n",
    "    comment = []\n",
    "    created_utc = []\n",
    "    score = []\n",
    "    ups = []\n",
    "    downs = []\n",
    "    controversiality = []\n",
    "    flair = []\n",
    "    gilded = []\n",
    "    over_18 = []\n",
    "    link = []\n",
    "    \n",
    "    for c in top:\n",
    "        subreddit.append(c.subreddit_name_prefixed)\n",
    "        comment.append(h.cleanText(c.body))\n",
    "        \n",
    "        parsed_date = datetime.utcfromtimestamp(c.created_utc)\n",
    "        year = parsed_date.year\n",
    "        month = parsed_date.month\n",
    "        day = parsed_date.day\n",
    "        \n",
    "        created_utc.append(parsed_date)\n",
    "        score.append(c.score)\n",
    "        ups.append(c.ups)\n",
    "        downs.append(c.downs)\n",
    "        controversiality.append(c.controversiality)\n",
    "        flair.append(c.author_flair_text)\n",
    "        gilded.append(c.gilded)\n",
    "        over_18.append(c.over_18)\n",
    "        link.append(c.link_permalink)\n",
    "        \n",
    "    for c in hot:\n",
    "        subreddit.append(c.subreddit_name_prefixed)\n",
    "        comment.append(h.cleanText(c.body))\n",
    "        \n",
    "        parsed_date = datetime.utcfromtimestamp(c.created_utc)\n",
    "        year = parsed_date.year\n",
    "        month = parsed_date.month\n",
    "        day = parsed_date.day\n",
    "        \n",
    "        created_utc.append(parsed_date)\n",
    "        score.append(c.score)\n",
    "        ups.append(c.ups)\n",
    "        downs.append(c.downs)\n",
    "        controversiality.append(c.controversiality)\n",
    "        flair.append(c.author_flair_text)\n",
    "        gilded.append(c.gilded)\n",
    "        over_18.append(c.over_18)\n",
    "        link.append(c.link_permalink)\n",
    "        \n",
    "    for c in contro:\n",
    "        subreddit.append(c.subreddit_name_prefixed)\n",
    "        comment.append(h.cleanText(c.body))\n",
    "        \n",
    "        parsed_date = datetime.utcfromtimestamp(c.created_utc)\n",
    "        year = parsed_date.year\n",
    "        month = parsed_date.month\n",
    "        day = parsed_date.day\n",
    "        \n",
    "        created_utc.append(parsed_date)\n",
    "        score.append(c.score)\n",
    "        ups.append(c.ups)\n",
    "        downs.append(c.downs)\n",
    "        controversiality.append(c.controversiality)\n",
    "        flair.append(c.author_flair_text)\n",
    "        gilded.append(c.gilded)\n",
    "        over_18.append(c.over_18)\n",
    "        link.append(c.link_permalink)\n",
    "        \n",
    "    df = pd.DataFrame(subreddit, columns = [\"subreddit\"])\n",
    "    df[\"comment\"] = comment\n",
    "    df[\"created_utc\"] = created_utc\n",
    "    df[\"score\"] = score\n",
    "    df[\"ups\"] = ups\n",
    "    df[\"downs\"] = downs\n",
    "    df[\"controversiality\"] = controversiality\n",
    "    df[\"flair\"] = flair\n",
    "    df[\"gilded\"] = gilded\n",
    "    df[\"over_18\"] = over_18\n",
    "    df[\"link\"] = link\n",
    "    \n",
    "    df = df.drop_duplicates(subset = [\"comment\"], keep = \"first\")    \n",
    "    print(\"Retrieved\", len(df), \"comments for user:\", redditor_name)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_comments = getRedditorInfo(\"Max_W_\", r)\n",
    "other_comments = getRedditorInfo(\"PrancingPeach\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find common subreddits between 2 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commonSubreddits(user1, user2, redditInstance):\n",
    "    \n",
    "    df1 = getRedditorInfo(user1, redditInstance)\n",
    "    df2 = getRedditorInfo(user2, redditInstance)\n",
    "    \n",
    "    df1 = df1.groupby([\"subreddit\"])[['comment']]\\\n",
    "                .count().reset_index()\\\n",
    "                .sort_values([\"comment\"], ascending = False)\n",
    "            \n",
    "    df2 = df2.groupby([\"subreddit\"])[['comment']]\\\n",
    "            .count().reset_index()\\\n",
    "            .sort_values([\"comment\"], ascending = False)\n",
    "            \n",
    "    df1 = df1.merge(df2, on = \"subreddit\", how = \"inner\")[\"subreddit\"]\n",
    "    result = np.array(df1)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commonSubredditCounts(user1, user2, redditInstance):\n",
    "    \n",
    "    common = commonSubreddits(user1, user2, redditInstance)  \n",
    "    \n",
    "    df1 = getRedditorInfo(user1, redditInstance)\n",
    "    df2 = getRedditorInfo(user2, redditInstance)\n",
    "    \n",
    "    df1 = df1[df1[\"subreddit\"].isin(common)]\n",
    "    df2 = df2[df2[\"subreddit\"].isin(common)]\n",
    "        \n",
    "    df1Counts = df1.groupby([\"subreddit\"])[['comment']]\\\n",
    "                .count().reset_index()\\\n",
    "                .sort_values([\"comment\"], ascending = False)\\\n",
    "                .reset_index(drop = True)\n",
    "                \n",
    "    df2Counts = df2.groupby([\"subreddit\"])[['comment']]\\\n",
    "            .count().reset_index()\\\n",
    "            .sort_values([\"comment\"], ascending = False)\\\n",
    "            .reset_index(drop = True)              \n",
    "                \n",
    "    return(df1Counts, df2Counts)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one, two = commonSubredditCounts(\"KCTigerGrad\", \"PrancingPeach\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get user flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFlairs(username, redditInstance):\n",
    "    df = getRedditorInfo(username, redditInstance)\n",
    "    flairs = set(np.unique(np.array((df[(df[\"flair\"].isnull() == False) & (df[\"flair\"] != \"\")][\"flair\"]))))\n",
    "    if (len(flairs) < 1):\n",
    "        print(\"No flairs for this user\")\n",
    "        return None\n",
    "    else:\n",
    "        return(flairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getFlairs(\"Max_W_\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFlairs(\"QiuYiDio\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n = 25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n = 25):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopFeatures(your_username, redditInstance):\n",
    "    \n",
    "    your_corpus = h.getUserComments(your_username, redditInstance)  \n",
    "    converted_corpus = []\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for c in your_corpus:        \n",
    "        tokens = nltk.word_tokenize(c)\n",
    "        new_c = ' '.join(stemmer.stem(t) for t in tokens) \n",
    "        converted_corpus.append(new_c)\n",
    "    \n",
    "    your_corpus = converted_corpus   \n",
    "    \n",
    "    myStopWords = stopwords.words('english')\n",
    "    myStopWords = text.ENGLISH_STOP_WORDS.union(myStopWords)\n",
    "    \n",
    "    tf1 = TfidfVectorizer(analyzer = \"word\", \n",
    "                        ngram_range = (1, 3),\n",
    "                        min_df = 0, \n",
    "                        stop_words = myStopWords)  \n",
    "    \n",
    "    matrix1 = tf1.fit_transform(your_corpus)\n",
    "    features1 = tf1.get_feature_names()         \n",
    "    yourTopOverallFeatures = top_mean_feats(matrix1, features1)\n",
    "        \n",
    "    return(yourTopOverallFeatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top = getTopFeatures(\"QiuYiDio\", r)\n",
    "top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top = getTopFeatures(\"ohai123456789\", r)\n",
    "top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find subreddits that a user posts in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSubredditsPosted(username, redditInstance):\n",
    "    comments = getRedditorInfo(username, redditInstance)\n",
    "    subs = np.array(comments[\"subreddit\"])\n",
    "    subs = (np.unique(subs))\n",
    "    return(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getSubredditsPosted(\"Max_W_\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze sentiment of user comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentiment(username, r):\n",
    "    comments = getRedditorInfo(username, r)\n",
    "                    \n",
    "    if (len(comments) < 1):\n",
    "        print(\"No comments for that user\")\n",
    "        return None\n",
    "                        \n",
    "    comments = comments.sort_values(\"created_utc\", ascending = True)    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    comments[\"negative\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"neg\"]) \n",
    "    comments[\"neutral\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"neu\"]) \n",
    "    comments[\"positive\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"pos\"]) \n",
    "    comments[\"compound\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"compound\"]) \n",
    "    \n",
    "    return(comments)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sents = getSentiment(\"Max_W_\", r)\n",
    "sents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sentiment of comments over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotSentiment(username, subreddit_name, redditInstance):\n",
    "    sents = getSentiment(username, redditInstance)\n",
    "    subreddit_name = \"r/\" + subreddit_name\n",
    "    sents = sents[sents[\"subreddit\"].str.lower() == subreddit_name.lower()]\n",
    "    sents = sents.sort_values(\"created_utc\", ascending = True)  \n",
    "    sents = sents.reset_index()\n",
    "    sents[\"id\"] = sents.index\n",
    "    \n",
    "    if (len(sents) < 3):\n",
    "        print(\"User has not posted on this subreddit\")\n",
    "        return        \n",
    "        \n",
    "    _ = plt.plot(sents[\"id\"], sents[\"compound\"], marker = \"\", linewidth = 1.9, alpha = 0.9)\n",
    "    title = \"Sentiment analysis on \" + subreddit_name + \" for user: \" + username\n",
    "    _ = plt.suptitle(title)\n",
    "    _ = plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotTopSentiments(username, redditInstance):\n",
    "    sents = getSentiment(username, redditInstance)    \n",
    "   \n",
    "    subreddits = np.array(sents.groupby([\"subreddit\"]).\\\n",
    "                                                  count().\\\n",
    "                                                  reset_index().\\\n",
    "                                                  sort_values(\"comment\", ascending = False).\\\n",
    "                                                  head(4)[\"subreddit\"])\n",
    "    \n",
    "    numberOfSubs = len(subreddits)\n",
    "    \n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('Set1')\n",
    "    num = 0\n",
    "    \n",
    "    for s in subreddits:       \n",
    "        \n",
    "        subreddit_name = s\n",
    "        df = sents[sents[\"subreddit\"].str.lower() == subreddit_name.lower()]       \n",
    "        \n",
    "        if (len(df) >= 2):            \n",
    "            num = num + 1 \n",
    "\n",
    "            # Find the right spot on the plot\n",
    "            _ = plt.subplot(2, 2, num)                 \n",
    "\n",
    "            df = df.sort_values(\"created_utc\", ascending = True)  \n",
    "            df = df.reset_index()\n",
    "            df[\"id\"] = df.index\n",
    "\n",
    "            _ = plt.plot(df[\"id\"], df[\"compound\"], marker = \"\", linewidth = 1.9, alpha = 0.9, color = palette(num))\n",
    "\n",
    "            # Not ticks everywhere\n",
    "            if num in range(7) :\n",
    "                _ = plt.tick_params(labelbottom='off')\n",
    "            if num not in [1,4,7] :\n",
    "                _ = plt.tick_params(labelleft='off')\n",
    "\n",
    "            # Add title\n",
    "            _ = plt.title(subreddit_name, loc='left', fontsize=12, fontweight=0, color=palette(num))\n",
    "             \n",
    "\n",
    "    title = \"Sentiment analysis \" + \"for user: \" + username\n",
    "    _ = plt.suptitle(title, fontsize=13, fontweight=0, color='black', style='italic', y=1.02)    \n",
    "    _ = plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotSentiment(\"Max_W_\", \"mizzou\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotTopSentiments(\"Max_W_\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for the validation visualizations, use D3.js\n",
    "# create word bubbles for both users (bigger = more comments posted)\n",
    "# check the favorites on DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# why makes a comment get a lot of upvotes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
