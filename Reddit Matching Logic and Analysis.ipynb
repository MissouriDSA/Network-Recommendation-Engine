{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import re, string\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import praw\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction import text \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "import json \n",
    "import math\n",
    "\n",
    "# set seaborn settings\n",
    "sns.set()\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True # set lines\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import credentials and helper functions\n",
    "import credentials as creds\n",
    "import helpers as h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# API AND LIBRARY DOCUMENTATIONS:\n",
    "# https://praw.readthedocs.io/en/latest/getting_started/\n",
    "# https://www.reddit.com/dev/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIENT_ID = creds.client_id()\n",
    "CLIENT_SECRET_KEY = creds.client_secret_key()\n",
    "\n",
    "\n",
    "r = praw.Reddit(client_id = CLIENT_ID,\n",
    "                client_secret = CLIENT_SECRET_KEY,\n",
    "                user_agent = 'RedditorMatch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the scraped datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, it takes quite some time to retrieve user comments from a particular subreddit. Remember, we're going into a specified subreddit, finding a list of users who posted, and then scraping out every single comment that those users made in the past. In order to remove this bottleneck from my demonstration, I have scraped the comments from users who have posted on r/mizzou. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scraped_subreddits = [\"mizzou\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation engine logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***find_similar():*** Takes in the TFIDF matrix (matrix), the index of the document that you want to match against (index), and the number of results to be displayed (top_n / optional). 6 is chosen as the default for top_n because we're really only interested in the top 5 matched redditors. However, your own username might be chosen as well because well....your comments are very similar to your own comments and thus would have the highest score. The similarity is calculated using the cosine similarity.\n",
    "\n",
    "* ***getDf():*** Takes in the string of the subreddit name and retrieves the scraped comments from the appropriate csv file. \n",
    "\n",
    "* ***stem():*** Takes in a corpus and returns a stemmed corpus. It first tokenizes and stems each word before putting it back together as a single document. \n",
    "\n",
    "* ***findMatches():*** Takes in your username (string), subreddit name (string), and reddit API instance. It calls getDf() to retrieve the scraped comments and applies it to a corpus of comments. It then calls one of the helper functions that I wrote (getUserComments()) to gather the comments of the input username and then prepends it onto the corpus array. It then builds the TFIDF vectorizer with the n_gram specificity of 1 to 3 words. In addition, I'm adding more stopwords into the mix. Once we fit and transform the corpus to get the matrix, we call \"find_similar()\" to retrieve the top 5 matched usernames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_similar(matrix, index, top_n = 6):\n",
    "    cosine_similarities = linear_kernel(matrix[index: index + 1], matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getDf(subreddit_name):\n",
    "    filePath = \"data/\" + subreddit_name + \".csv\"\n",
    "    df = pd.read_csv(filePath, encoding = \"ISO-8859-1\")\n",
    "    print(\"--- Retrieved\", len(df), \"corpuses/corpi(?) for\", subreddit_name)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stem(corpus):\n",
    "    newCorpus = []\n",
    "    print(\"------ Stemming the words\")\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for c in corpus:\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(c)\n",
    "            \n",
    "            new_tokens = []\n",
    "            \n",
    "            for t in tokens:\n",
    "                try:\n",
    "                    new_t = stemmer.stem(t)\n",
    "                    new_tokens.append(new_t)          \n",
    "                except:\n",
    "                    print(\"can't stem the word. moving on...\")\n",
    "\n",
    "            new_c = ' '.join(new_tokens)\n",
    "            newCorpus.append(new_c)\n",
    "                \n",
    "        except:\n",
    "            print(\"skipping the comment. something went wrong...\")\n",
    "        \n",
    "    return(newCorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMatches(your_username, subreddit_name, redditInstance):\n",
    "    corpus = []    \n",
    "    corpusDf = getDf(subreddit_name)\n",
    "    corpusDf[\"Comments\"].apply(lambda row: corpus.append(str(row)))\n",
    "    \n",
    "    corpus = stem(corpus)\n",
    "    \n",
    "    your_comments = h.getUserComments(your_username, redditInstance)\n",
    "    your_comments = stem(your_comments)    \n",
    "    your_comments = \" \".join(your_comments)\n",
    "    \n",
    "    corpus.insert(0, your_comments)\n",
    "    \n",
    "    print(\"--- Creating Tfidf vector...\")\n",
    "    \n",
    "    myStopWords = stopwords.words('english')\n",
    "    myStopWords = text.ENGLISH_STOP_WORDS.union(myStopWords)\n",
    "    \n",
    "    tf = TfidfVectorizer(analyzer = \"word\", max_df = 0.8, min_df = 0.2, \n",
    "                            ngram_range = (1, 3),\n",
    "                            stop_words = myStopWords)\n",
    "    \n",
    "    print(\"--- Fitting the matrix...\")\n",
    "    matrix = tf.fit_transform(corpus)\n",
    "    results = []\n",
    "    \n",
    "    for index, score in find_similar(matrix, 0):        \n",
    "        index = index - 1 # because we prepended our comments onto the corpus, the index number was shifted by 1.\n",
    "        user = corpusDf.iloc[index, 0]\n",
    "        results.append(user)\n",
    "        print(\"...\")\n",
    "        print(\"...\")\n",
    "        print(\"Username:\", user, \"| Score:\", score)\n",
    "        print(\"=========================================================\")\n",
    "        \n",
    "    return(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: I'm displaying the top 6 usernames who are similar. \n",
    "# I chose 6 because if the username is contained in my scraped dataset, \n",
    "# then that username will always show up as most similar. \n",
    "# But we really only care about the top 5 usernames. \n",
    "\n",
    "matches = findMatches(\"Max_W_\", scraped_subreddits[0], r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gather user information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRedditorInfo(redditor_name, r):\n",
    "    user = r.redditor(redditor_name)\n",
    "    top = user.comments.top(limit = 1000)\n",
    "    hot = user.comments.hot(limit = 1000)\n",
    "    contro = user.comments.controversial(limit = 1000)\n",
    "    \n",
    "    subreddit = []\n",
    "    comment = []\n",
    "    created_utc = []\n",
    "    score = []\n",
    "    ups = []\n",
    "    downs = []\n",
    "    controversiality = []\n",
    "    flair = []\n",
    "    gilded = []\n",
    "    over_18 = []\n",
    "    link = []\n",
    "    \n",
    "    for c in top:\n",
    "        subreddit.append(c.subreddit_name_prefixed)\n",
    "        comment.append(h.cleanText(c.body))\n",
    "        \n",
    "        parsed_date = datetime.utcfromtimestamp(c.created_utc)\n",
    "        year = parsed_date.year\n",
    "        month = parsed_date.month\n",
    "        day = parsed_date.day\n",
    "        \n",
    "        created_utc.append(parsed_date)\n",
    "        score.append(c.score)\n",
    "        ups.append(c.ups)\n",
    "        downs.append(c.downs)\n",
    "        controversiality.append(c.controversiality)\n",
    "        flair.append(c.author_flair_text)\n",
    "        gilded.append(c.gilded)\n",
    "        over_18.append(c.over_18)\n",
    "        link.append(c.link_permalink)\n",
    "        \n",
    "    for c in hot:\n",
    "        subreddit.append(c.subreddit_name_prefixed)\n",
    "        comment.append(h.cleanText(c.body))\n",
    "        \n",
    "        parsed_date = datetime.utcfromtimestamp(c.created_utc)\n",
    "        year = parsed_date.year\n",
    "        month = parsed_date.month\n",
    "        day = parsed_date.day\n",
    "        \n",
    "        created_utc.append(parsed_date)\n",
    "        score.append(c.score)\n",
    "        ups.append(c.ups)\n",
    "        downs.append(c.downs)\n",
    "        controversiality.append(c.controversiality)\n",
    "        flair.append(c.author_flair_text)\n",
    "        gilded.append(c.gilded)\n",
    "        over_18.append(c.over_18)\n",
    "        link.append(c.link_permalink)\n",
    "        \n",
    "    for c in contro:\n",
    "        subreddit.append(c.subreddit_name_prefixed)\n",
    "        comment.append(h.cleanText(c.body))\n",
    "        \n",
    "        parsed_date = datetime.utcfromtimestamp(c.created_utc)\n",
    "        year = parsed_date.year\n",
    "        month = parsed_date.month\n",
    "        day = parsed_date.day\n",
    "        \n",
    "        created_utc.append(parsed_date)\n",
    "        score.append(c.score)\n",
    "        ups.append(c.ups)\n",
    "        downs.append(c.downs)\n",
    "        controversiality.append(c.controversiality)\n",
    "        flair.append(c.author_flair_text)\n",
    "        gilded.append(c.gilded)\n",
    "        over_18.append(c.over_18)\n",
    "        link.append(c.link_permalink)\n",
    "        \n",
    "    df = pd.DataFrame(subreddit, columns = [\"subreddit\"])\n",
    "    df[\"comment\"] = comment\n",
    "    df[\"created_utc\"] = created_utc\n",
    "    df[\"score\"] = score\n",
    "    df[\"ups\"] = ups\n",
    "    df[\"downs\"] = downs\n",
    "    df[\"controversiality\"] = controversiality\n",
    "    df[\"flair\"] = flair\n",
    "    df[\"gilded\"] = gilded\n",
    "    df[\"over_18\"] = over_18\n",
    "    df[\"link\"] = link\n",
    "    \n",
    "    df = df.drop_duplicates(subset = [\"comment\"], keep = \"first\")    \n",
    "    print(\"Retrieved\", len(df), \"comments for user:\", redditor_name)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments1 = getRedditorInfo(\"Max_W_\", r)\n",
    "comments2 = getRedditorInfo(\"BrettGilpin\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find common subreddits between 2 users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commonSubreddits(user1, user2, redditInstance):\n",
    "    \n",
    "    df1 = getRedditorInfo(user1, redditInstance)\n",
    "    df2 = getRedditorInfo(user2, redditInstance)\n",
    "    \n",
    "    df1 = df1.groupby([\"subreddit\"])[['comment']]\\\n",
    "                .count().reset_index()\\\n",
    "                .sort_values([\"comment\"], ascending = False)\n",
    "            \n",
    "    df2 = df2.groupby([\"subreddit\"])[['comment']]\\\n",
    "            .count().reset_index()\\\n",
    "            .sort_values([\"comment\"], ascending = False)\n",
    "            \n",
    "    df1 = df1.merge(df2, on = \"subreddit\", how = \"inner\")[\"subreddit\"]\n",
    "    result = np.array(df1)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def commonSubredditCounts(user1, user2, redditInstance):\n",
    "    \n",
    "    common = commonSubreddits(user1, user2, redditInstance)  \n",
    "    print(len(common), \"common subreddits found...\")\n",
    "    \n",
    "    df1 = getRedditorInfo(user1, redditInstance)\n",
    "    df2 = getRedditorInfo(user2, redditInstance)\n",
    "    \n",
    "    df1 = df1[df1[\"subreddit\"].isin(common)]\n",
    "    df2 = df2[df2[\"subreddit\"].isin(common)]\n",
    "        \n",
    "    df1Counts = df1.groupby([\"subreddit\"])[['comment']]\\\n",
    "                .count().reset_index()\\\n",
    "                .sort_values([\"comment\"], ascending = False)\\\n",
    "                .reset_index(drop = True)\n",
    "                \n",
    "    df2Counts = df2.groupby([\"subreddit\"])[['comment']]\\\n",
    "            .count().reset_index()\\\n",
    "            .sort_values([\"comment\"], ascending = False)\\\n",
    "            .reset_index(drop = True)      \n",
    "            \n",
    "    renameCols = [\"id\", \"value\"]\n",
    "            \n",
    "    df1Counts.columns = renameCols\n",
    "    df2Counts.columns = renameCols\n",
    "    \n",
    "    df1Counts[\"id\"] = df1Counts[\"id\"].str.lower()\n",
    "    df2Counts[\"id\"] = df2Counts[\"id\"].str.lower()\n",
    "    \n",
    "    df1Counts[\"id\"] = df1Counts[\"id\"].str.replace(\"r/\", \"\")\n",
    "    df2Counts[\"id\"] = df2Counts[\"id\"].str.replace(\"r/\", \"\")\n",
    "    \n",
    "    df1Counts = df1Counts.sort_values(by = [\"id\"], ascending = True)\n",
    "    df2Counts = df2Counts.sort_values(by = [\"id\"], ascending = True)    \n",
    "    \n",
    "                \n",
    "    return(df1Counts, df2Counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subredditCounts(user, redditInstance):\n",
    "    df1 = getRedditorInfo(user, redditInstance)\n",
    "    \n",
    "    df1Counts = df1.groupby([\"subreddit\"])[['comment']]\\\n",
    "                .count().reset_index()\\\n",
    "                .sort_values([\"comment\"], ascending = False)\\\n",
    "                .reset_index(drop = True)\n",
    "                \n",
    "    renameCols = [\"id\", \"value\"]\n",
    "    df1Counts.columns = renameCols\n",
    "    df1Counts[\"id\"] = df1Counts[\"id\"].str.lower()\n",
    "    df1Counts[\"id\"] = df1Counts[\"id\"].str.replace(\"r/\", \"\")\n",
    "    df1Counts = df1Counts.sort_values(by = [\"id\"], ascending = True)   \n",
    "    \n",
    "    return(df1Counts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allSubreddits1 = subredditCounts(\"Max_W_\", r)\n",
    "allSubreddits2 = subredditCounts(\"PrancingPeach\", r)\n",
    "\n",
    "# Output to CSV files for visualization. \n",
    "allSubreddits1.to_csv(\"frontend/all_sub1.csv\", index = False)\n",
    "allSubreddits2.to_csv(\"frontend/all_sub2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one, two = commonSubredditCounts(\"Max_W_\", \"BrettGilpin\", r)\n",
    "\n",
    "# Output to CSV files for visualization. \n",
    "one.to_csv(\"frontend/common_sub1.csv\", index = False)\n",
    "two.to_csv(\"frontend/common_sub2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get user flairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFlairs(username, redditInstance):\n",
    "    df = getRedditorInfo(username, redditInstance)\n",
    "    flairs = set(np.unique(np.array((df[(df[\"flair\"].isnull() == False) & (df[\"flair\"] != \"\")][\"flair\"]))))\n",
    "    if (len(flairs) < 1):\n",
    "        print(\"No flairs for this user\")\n",
    "        return None\n",
    "    else:\n",
    "        return(flairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "getFlairs(\"Max_W_\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getFlairs(\"BrettGilpin\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find top features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n = 50):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_feats_in_doc(Xtr, features, row_id, top_n = 50):\n",
    "    ''' Top tfidf features in specific document (matrix row) '''\n",
    "    row = np.squeeze(Xtr[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.1, top_n=50):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getTopFeatures(matches, redditInstance):\n",
    "    \n",
    "    myStopWords = stopwords.words('english')\n",
    "    myStopWords = text.ENGLISH_STOP_WORDS.union(myStopWords)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    for matched in matches:\n",
    "        print(\"------ Working on\", matched)\n",
    "        allCorpuses = []\n",
    "        other_comments = h.getUserComments(matched, redditInstance)\n",
    "        \n",
    "        for oc in other_comments:\n",
    "            tokens = nltk.word_tokenize(oc)\n",
    "            new_oc = ' '.join(stemmer.stem(t) for t in tokens)\n",
    "            allCorpuses.append(new_oc)\n",
    "    \n",
    "        tf1 = TfidfVectorizer(analyzer = \"word\",\n",
    "                        ngram_range = (1, 3),\n",
    "                        stop_words = myStopWords)  \n",
    "            \n",
    "        print(\"------ Fitting and tranforming the TFIDF vectorizer\")\n",
    "        matrix1 = tf1.fit_transform(allCorpuses)\n",
    "        features1 = tf1.get_feature_names()\n",
    "        \n",
    "        topFeatures = top_mean_feats(matrix1, features1)\n",
    "        cols = [\"features\", \"mean score\"]\n",
    "        topFeatures.columns = cols\n",
    "        print(\"\")\n",
    "        print(topFeatures.head(20))\n",
    "        print(\"-----------------------------------------------------------------------\")\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "top = getTopFeatures(matches, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures(your_username, redditInstance):\n",
    "    \n",
    "    myStopWords = stopwords.words('english')\n",
    "    myStopWords = text.ENGLISH_STOP_WORDS.union(myStopWords)\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    allCorpuses = []\n",
    "    \n",
    "    your_comments = h.getUserComments(your_username, redditInstance)\n",
    "    your_corpus = []\n",
    "    for yc in your_comments:\n",
    "        tokens = nltk.word_tokenize(yc)\n",
    "        new_yc = ' '.join(stemmer.stem(t) for t in tokens)\n",
    "        your_corpus.append(new_yc)\n",
    "    \n",
    "    tf = TfidfVectorizer(analyzer = \"word\",\n",
    "                        ngram_range = (1, 3),\n",
    "                        stop_words = myStopWords) \n",
    "    \n",
    "    matrix = tf.fit_transform(your_corpus)\n",
    "    features = tf.get_feature_names()\n",
    "    idf = tf.idf_\n",
    "    result = dict(zip(features, idf))\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCommonFeatures(user1, user2, redditInstance):\n",
    "    f1 = getFeatures(user1, redditInstance)\n",
    "    f2 = getFeatures(user2, redditInstance)\n",
    "    \n",
    "    res1 = sorted(((value, key) for (key, value) in f1.items()), reverse = True)[0:5000]\n",
    "    res2 = sorted(((value, key) for (key, value) in f2.items()), reverse = True)[0:5000]\n",
    "    \n",
    "    set1 = set(t[1] for t in res1)\n",
    "    set2 = set(t[1] for t in res2)\n",
    "    \n",
    "    commonFeats = set1 & set2\n",
    "    return(commonFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "getCommonFeatures(\"Max_W_\", \"BrettGilpin\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find subreddits that a user posts in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSubredditsPosted(username, redditInstance):\n",
    "    comments = getRedditorInfo(username, redditInstance)\n",
    "    subs = np.array(comments[\"subreddit\"])\n",
    "    subs = (np.unique(subs))\n",
    "    return(subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getSubredditsPosted(\"Max_W_\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze sentiment of user comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getSentiment(username, r):\n",
    "    comments = getRedditorInfo(username, r)\n",
    "                    \n",
    "    if (len(comments) < 1):\n",
    "        print(\"No comments for that user\")\n",
    "        return None\n",
    "                        \n",
    "    comments = comments.sort_values(\"created_utc\", ascending = True)    \n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    comments[\"negative\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"neg\"]) \n",
    "    comments[\"neutral\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"neu\"]) \n",
    "    comments[\"positive\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"pos\"]) \n",
    "    comments[\"compound\"] = comments[\"comment\"].apply(lambda x: sid.polarity_scores(x)[\"compound\"]) \n",
    "    \n",
    "    return(comments)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = getSentiment(\"Max_W_\", r)\n",
    "sents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot sentiment of comments over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotSentiment(username, subreddit_name, redditInstance):\n",
    "    sents = getSentiment(username, redditInstance)\n",
    "    subreddit_name = \"r/\" + subreddit_name\n",
    "    sents = sents[sents[\"subreddit\"].str.lower() == subreddit_name.lower()]\n",
    "    sents = sents.sort_values(\"created_utc\", ascending = True)  \n",
    "    sents = sents.reset_index()\n",
    "    sents[\"id\"] = sents.index\n",
    "    \n",
    "    if (len(sents) < 3):\n",
    "        print(\"User has not posted on this subreddit\")\n",
    "        return        \n",
    "        \n",
    "    _ = plt.plot(sents[\"id\"], sents[\"compound\"], marker = \"\", linewidth = 1.9, alpha = 0.9)\n",
    "    title = \"Sentiment analysis on \" + subreddit_name + \" for user: \" + username\n",
    "    _ = plt.suptitle(title)\n",
    "    _ = plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plotTopSentiments(username, redditInstance):\n",
    "    sents = getSentiment(username, redditInstance)    \n",
    "   \n",
    "    subreddits = np.array(sents.groupby([\"subreddit\"]).\\\n",
    "                                                  count().\\\n",
    "                                                  reset_index().\\\n",
    "                                                  sort_values(\"comment\", ascending = False).\\\n",
    "                                                  head(4)[\"subreddit\"])\n",
    "    \n",
    "    numberOfSubs = len(subreddits)\n",
    "    \n",
    "    # create a color palette\n",
    "    palette = plt.get_cmap('Set1')\n",
    "    num = 0\n",
    "    \n",
    "    for s in subreddits:       \n",
    "        \n",
    "        subreddit_name = s\n",
    "        df = sents[sents[\"subreddit\"].str.lower() == subreddit_name.lower()]       \n",
    "        \n",
    "        if (len(df) >= 2):            \n",
    "            num = num + 1 \n",
    "\n",
    "            # Find the right spot on the plot\n",
    "            _ = plt.subplot(2, 2, num)                 \n",
    "\n",
    "            df = df.sort_values(\"created_utc\", ascending = True)  \n",
    "            df = df.reset_index()\n",
    "            df[\"id\"] = df.index\n",
    "\n",
    "            _ = plt.plot(df[\"id\"], df[\"compound\"], marker = \"\", linewidth = 1.9, alpha = 0.9, color = palette(num))\n",
    "\n",
    "            # Not ticks everywhere\n",
    "            if num in range(7) :\n",
    "                _ = plt.tick_params(labelbottom='off')\n",
    "            if num not in [1,4,7] :\n",
    "                _ = plt.tick_params(labelleft='off')\n",
    "\n",
    "            # Add title\n",
    "            _ = plt.title(subreddit_name, loc='left', fontsize=12, fontweight=0, color=palette(num))\n",
    "             \n",
    "\n",
    "    title = \"Sentiment analysis \" + \"for user: \" + username\n",
    "    _ = plt.suptitle(title, fontsize=13, fontweight=0, color='black', style='italic', y=1.02)    \n",
    "    _ = plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotSentiment(\"Max_W_\", \"mizzou\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotTopSentiments(\"Max_W_\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plotTopSentiments(\"BrettGilpin\", r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: plot a cumulative chart of sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build collapsible index visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def formatToDict(name, size):    \n",
    "    return {'name': name, 'size': round(size*100000)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildCollapsible(your_username, subreddit_name, redditInstance):\n",
    "    matches = list(findMatches(your_username, subreddit_name, redditInstance))\n",
    "    \n",
    "    if (len(matches) < 1): \n",
    "        return None\n",
    "    \n",
    "    entireObjDict = {}\n",
    "    entireObjDict[\"name\"] = \"Top Features\"\n",
    "    objList = []\n",
    "    \n",
    "    for username in matches:\n",
    "        colNames = [\"name\", \"size\"]\n",
    "        features = getTopFeatures(username, redditInstance)        \n",
    "        features.columns = colNames\n",
    "        features = features.head(15)\n",
    "        \n",
    "        children = list(features.apply(lambda row: formatToDict(row[\"name\"], row[\"size\"]), axis = 1))\n",
    "        children\n",
    "        \n",
    "        userObj = {}\n",
    "        userObj[\"name\"] = username\n",
    "        userObj[\"children\"] = children\n",
    "        objList.append(userObj)\n",
    "        \n",
    "    entireObjDict[\"children\"] = objList        \n",
    "    return(entireObjDict)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonObj = buildCollapsible(\"Max_W_\", \"mizzou\", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
